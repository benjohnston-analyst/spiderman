{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# West Sussex webscraper\n",
    "\n",
    "Created as a fun project to play around with webscraping and trying to identify dead links on the west sussex connect site\n",
    "\n",
    "https://www.westsussexconnecttosupport.org/\n",
    "\n",
    "### To Do\n",
    "- Refine the flagging of dead links\n",
    "    - Investigate why some of the links have the west sussex gov url repeated at the end\n",
    "- Speed up the webcrawler\n",
    "- Create an output of url trees\n",
    "\n",
    "### Stretch goals\n",
    "- Create a front end and have this be an agnostic tool for looking at any website witha front end output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_url = 'https://www.westsussexconnecttosupport.org/'  # Replace with the website's root URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_internal_links(url):\n",
    "    \"\"\"Fetch all internal links from the given URL within the specified domain.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract all href links\n",
    "        excluded_prefixes = ('tel:', 'mailto:', 'Tel:', 'Mailto:', 'email:', 'Email:')\n",
    "        links = [a.get('href') for a in soup.find_all('a', href=True) if not a.get('href').startswith(excluded_prefixes)]\n",
    "\n",
    "        # Filter internal links only\n",
    "        internal_links = []\n",
    "        for link in links:\n",
    "            full_url = urljoin(url, link)  # Ensure full URL for relative links\n",
    "            internal_links.append(full_url)\n",
    "        \n",
    "        return list(set(internal_links))  # Remove duplicates\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error accessing {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "def crawl_website(start_url):\n",
    "    \"\"\"Crawl the website and create a dictionary of each URL and its internal links. Track external URLs without visiting.\"\"\"\n",
    "    domain = urlparse(start_url).netloc\n",
    "    visited_urls = set()  # Track visited internal URLs to avoid cycles\n",
    "    visited_external_urls = set() # Track visited external URLs\n",
    "    url_dict = {}         # Dictionary to store URLs and their internal links\n",
    "    to_visit = [start_url]  # Start with the root URL\n",
    "    \n",
    "    with tqdm(total=len(to_visit), desc=\"Crawling\", unit=\"page\") as pbar:\n",
    "        while to_visit:\n",
    "            current_url = to_visit.pop()\n",
    "            parsed_current_url = urlparse(current_url)\n",
    "\n",
    "            if parsed_current_url.netloc == domain:\n",
    "                if current_url not in visited_urls:\n",
    "                    visited_urls.add(current_url)\n",
    "                    \n",
    "                    # Get internal links on the current page\n",
    "                    internal_links = get_internal_links(current_url)\n",
    "                    url_dict[current_url] = internal_links\n",
    "                    \n",
    "                    # Add new URLs to visit that haven't been visited\n",
    "                    for link in internal_links:\n",
    "                        if link not in visited_urls:\n",
    "                            to_visit.append(link)\n",
    "\n",
    "            else:\n",
    "                # Marking external links as visited with scraping\n",
    "                if current_url not in visited_external_urls:\n",
    "                    visited_external_urls.add(current_url)\n",
    "                    url_dict[current_url] = []\n",
    "\n",
    "            pbar.update(1)\n",
    "            pbar.total = len(to_visit) + len(visited_urls) + len(visited_external_urls) + 1\n",
    "            pbar.refresh()\n",
    "            \n",
    "            time.sleep(0.5)  # Being polite to the server\n",
    "\n",
    "    return url_dict\n",
    "\n",
    "url_links = crawl_website(start_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_404_pages(url_dict):\n",
    "    \"\"\"Check each URL in the dictionary for 'Page not found' or '404' error messages.\"\"\"\n",
    "    not_found_pages = [] # Creating list of not found pages and their originating URL\n",
    "    \n",
    "    for origin_url, links in tqdm(url_dict.items(), desc=\"Checking for 'Page not found'\"):\n",
    "        for link in links:\n",
    "            try:\n",
    "                response = requests.get(link)\n",
    "                if response.status_code == 404 or 'page not found' in response.text.lower():\n",
    "                    not_found_pages.append((origin_url, link))\n",
    "\n",
    "            except requests.RequestException as e:\n",
    "                not_found_pages.append((origin_url,link))\n",
    "\n",
    "        time.sleep(0.5) # Being polite to the server\n",
    "    \n",
    "    return not_found_pages\n",
    "\n",
    "not_found_urls = check_for_404_pages(url_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(not_found_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_found_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the url_links dictionary into a pandas dataframe\n",
    "data = [(url, internal_link) for url, links in url_links.items() for internal_link in (links if links else [None])]\n",
    "\n",
    "df = pd.DataFrame(data, columns=[\"URL\", \"Internal_Link\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the not_found_urls into a pandas dataframe\n",
    "broken_links = pd.DataFrame(not_found_urls, columns=[\"Originating_URL\", \"Flagged_URL\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broken_links.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "broken_links.to_csv('../playarea/west_sussex_broken_links.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
